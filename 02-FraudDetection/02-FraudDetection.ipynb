{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "## Solution\n",
    "In order to solve the problem \n",
    "- Which is provide a classification method that correctly states whether a signature can be a fraud/genuine/disguised type.\n",
    "- **I propose 2 Deep Learning classification model using a Shallow model and a MiniVGG model that are two kinds of CNN arquitecture.**\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nury/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pyimagesearch.preprocessing import ImageToArrayPreprocessor\n",
    "from pyimagesearch.preprocessing import AspectAwarePreprocessor\n",
    "from pyimagesearch.datasets import SimpleDatasetLoader\n",
    "from pyimagesearch.nn.conv import ShallowNet\n",
    "from pyimagesearch.nn.conv import MiniVGGNet\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The objetive of the challenge is classify a signature in one of these 3 classes (Genuine, Disguised, Simulated). For these reason I only work with these 3 classes.\n",
    "\n",
    "### Readind the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'TrainingSet'\n",
    "image_paths = list(paths.list_images(dataset_path))\n",
    "class_names = [pt.split(os.path.sep)[-2] for pt in image_paths]\n",
    "class_names = [str(x) for x in np.unique(class_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- 1:  Genuine\n",
    "- 2:  Disguised\n",
    "- 3:  Simulated(fraud)\n",
    "\n",
    "Some example of images path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingSet/2/D037.png',\n",
       " 'TrainingSet/2/D114.png',\n",
       " 'TrainingSet/1/G111.png',\n",
       " 'TrainingSet/1/G097.png',\n",
       " 'TrainingSet/1/G003.png',\n",
       " 'TrainingSet/1/G102.png',\n",
       " 'TrainingSet/1/G172.png',\n",
       " 'TrainingSet/1/G084.png',\n",
       " 'TrainingSet/1/G015.png',\n",
       " 'TrainingSet/1/G187.png',\n",
       " 'TrainingSet/3/S049.png',\n",
       " 'TrainingSet/3/S138.png',\n",
       " 'TrainingSet/3/S032.png',\n",
       " 'TrainingSet/3/S081.png',\n",
       " 'TrainingSet/3/S196.png',\n",
       " 'TrainingSet/3/S142.png',\n",
       " 'TrainingSet/3/S145.png',\n",
       " 'TrainingSet/3/S022.png',\n",
       " 'TrainingSet/3/S028.png',\n",
       " 'TrainingSet/3/S008.png']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all shape of image to (128,90) and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 128\n",
    "height = 90\n",
    "aap = AspectAwarePreprocessor(width, height)\n",
    "iap = ImageToArrayPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 50/200\n",
      "[INFO] processed 100/200\n",
      "[INFO] processed 150/200\n",
      "[INFO] processed 200/200\n"
     ]
    }
   ],
   "source": [
    "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
    "(data, labels) = sdl.load(image_paths, verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in train and test. 30% of dataset is for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of images for training and for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  140\n",
      "Testing images:  60\n"
     ]
    }
   ],
   "source": [
    "print(\"Training images: \", len(trainX))\n",
    "print(\"Testing images: \", len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of images for each class for training and for testing. We have 30% of images of each class for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'3': 73, '1': 53, '2': 14})\n",
      "Total: 140\n"
     ]
    }
   ],
   "source": [
    "print(Counter(trainY))\n",
    "print(\"Total:\", len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'3': 31, '1': 23, '2': 6})\n",
      "Total: 60\n"
     ]
    }
   ],
   "source": [
    "print(Counter(testY))\n",
    "print(\"Total:\", len(testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the labels into output standar of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_b = LabelBinarizer().fit_transform(trainY)\n",
    "testY_b = LabelBinarizer().fit_transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, these 2 ways of labels represent the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3' '3' '1' '2' '1']\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY[:5])\n",
    "print(trainY_b[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced class\n",
    "The dataset doesn't has the same number of images to each class (reference, disguised, genuine and simulated). In orden to solve this problem I **modify the cost function of models to balance the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done using a class_weights function that get a parameters that is inversely proportional to number of instances of each classes. In our case we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(trainY), trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88050314, 3.33333333, 0.63926941])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get these parameters for each classes:\n",
    "- 1:  Genuine --> 0.88050314\n",
    "- 2:  Disguised --> 3.3333333\n",
    "- 3:  Simulated --> 0.63926941\n",
    "\n",
    "That is taken into consideration in the cost function of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Shallow Model\n",
    "Training this model with 5 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Train on 140 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "140/140 [==============================] - 4s 27ms/step - loss: 7.6162 - acc: 0.5214 - val_loss: 7.7904 - val_acc: 0.5167\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 4s 31ms/step - loss: 7.7137 - acc: 0.5214 - val_loss: 7.7904 - val_acc: 0.5167\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 4s 29ms/step - loss: 7.7137 - acc: 0.5214 - val_loss: 7.7904 - val_acc: 0.5167\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 4s 26ms/step - loss: 7.7137 - acc: 0.5214 - val_loss: 7.7904 - val_acc: 0.5167\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 3s 24ms/step - loss: 7.7137 - acc: 0.5214 - val_loss: 7.7904 - val_acc: 0.5167\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "optShallowNet = SGD(lr=0.005)\n",
    "modelShallowNet = ShallowNet.build(width=width, height=height, depth=1, classes=3)\n",
    "modelShallowNet.compile(loss=\"categorical_crossentropy\", optimizer=optShallowNet,\n",
    "metrics=[\"accuracy\"])\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "EPOCH = 5\n",
    "H_ShallowNet = modelShallowNet.fit(trainX, trainY_b, validation_data=(testX, testY_b), class_weight=class_weights,\n",
    "batch_size=2, epochs=EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 90, 128, 32)       320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 90, 128, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 368640)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1105923   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,106,243\n",
      "Trainable params: 1,106,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelShallowNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.52      1.00      0.68        31\n",
      "\n",
      "    accuracy                           0.52        60\n",
      "   macro avg       0.17      0.33      0.23        60\n",
      "weighted avg       0.27      0.52      0.35        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nury/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictionsmodelShallowNet = modelShallowNet.predict(testX, batch_size=2)\n",
    "print(classification_report(testY_b.argmax(axis=1), predictionsmodelShallowNet.argmax(axis=1), target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 23],\n",
       "       [ 0,  0,  6],\n",
       "       [ 0,  0, 31]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(testY_b.argmax(axis=1), predictionsmodelShallowNet.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not good because all image signature is classified as **simulated(fraud)**. Some reasons of that is the few numbers of images for each class.\n",
    "\n",
    "#### Possible solutons\n",
    "- Data augmentation: Increase the number of images for each classes in order to have more samples and to get balanced dataset.\n",
    "- Use more powerfull CNN model\n",
    "\n",
    "I decided to propose a second model called **MiniVGG** that is more powerfull CNN model.\n",
    "\n",
    "### MiniVGG model\n",
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Train on 140 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "140/140 [==============================] - 60s 430ms/step - loss: 3.0016 - acc: 0.4786 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 74s 525ms/step - loss: 3.9608 - acc: 0.4643 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 76s 541ms/step - loss: 3.8166 - acc: 0.5429 - val_loss: 8.1713 - val_acc: 0.4833\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 82s 584ms/step - loss: 3.9134 - acc: 0.5000 - val_loss: 7.4328 - val_acc: 0.5167\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 60s 431ms/step - loss: 3.7080 - acc: 0.4714 - val_loss: 6.5058 - val_acc: 0.4333\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=0.05)\n",
    "model = MiniVGGNet.build(width=width, height=height, depth=1, classes=len(class_names))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX, trainY_b, validation_data=(testX, testY_b), batch_size=2, epochs=EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      1.00      0.59        23\n",
      "           2       0.67      0.33      0.44         6\n",
      "           3       0.50      0.03      0.06        31\n",
      "\n",
      "    accuracy                           0.43        60\n",
      "   macro avg       0.53      0.46      0.36        60\n",
      "weighted avg       0.49      0.43      0.30        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictionsmodelMiniVGG = model.predict(testX, batch_size=2)\n",
    "print(classification_report(testY_b.argmax(axis=1), predictionsmodelMiniVGG.argmax(axis=1), target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  0,  0],\n",
       "       [ 3,  2,  1],\n",
       "       [29,  1,  1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(testY_b.argmax(axis=1), predictionsmodelMiniVGG.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are better that previous, all genuine signature are classified correctly, however of 6 disguised signature 3 of them are classified as genuine and 1 as fraud. Of 31 simulated(fraud) images only one is classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Data augmentation is neccesary to get more samples of each classes.\n",
    "- MiniVGG is my baseline model but It could be more tuning to adapt to this problem.\n",
    "- Another technique the could perform is Matching signature based of distance metric. For example use reference signature like our pattern and doing matching with other kind of signature like genuine or fraud. The fraud should have more distance respect to reference signature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save H5 model file\n",
    "it is saving the miniVGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model.save(\"modelMiniVGGNet.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
