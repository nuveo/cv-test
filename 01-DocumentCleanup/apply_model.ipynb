{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np, random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import ConvexHull\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aponte os diretórios com as imagens a serem processadas e o diretório de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/'\n",
    "data_dir = './sample_data'\n",
    "out_dir = './sample_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo temos a definição da Unet modificada para também retornar um feature map intermediário que é utilizado como entrada pelo modelo de segmentação de texto. Eu escolhi essa modificação por que no momento que eu optei por criar também a segmentação de texto eu ja tinha os pesos para a Unet original bem treinados ,assim sendo eu não queria começar todo o treinamento do Zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetVgg(torch.nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(UNetVgg, self).__init__()\n",
    "\n",
    "        vgg16pre = torchvision.models.vgg16(pretrained=True)\n",
    "        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n",
    "        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n",
    "        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n",
    "        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n",
    "        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n",
    "\n",
    "\n",
    "\n",
    "        self.smooth0 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(256, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "        self.smooth3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "                torch.nn.ReLU(True)\n",
    "                )\n",
    "\n",
    "        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        feat0 = self.vgg0(x)\n",
    "        feat1 = self.vgg1(feat0)\n",
    "        feat2 = self.vgg2(feat1)\n",
    "        feat3 = self.vgg3(feat2)\n",
    "        feat4 = self.vgg4(feat3)\n",
    "\n",
    "        _,_,H,W = feat3.size()\n",
    "        up3 = torch.nn.functional.interpolate(feat4, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat3 = torch.cat([feat3, up3], 1)\n",
    "        end3 = self.smooth3(concat3)\n",
    "\n",
    "        _,_,H,W = feat2.size()\n",
    "        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat2 = torch.cat([feat2, up2], 1)\n",
    "        end2 = self.smooth2(concat2)\n",
    "\n",
    "        _,_,H,W = feat1.size()\n",
    "        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat1 = torch.cat([feat1, up1], 1)\n",
    "        end1 = self.smooth1(concat1)\n",
    "\n",
    "        _,_,H,W = feat0.size()\n",
    "        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=False)\n",
    "        concat0 = torch.cat([feat0, up0], 1)\n",
    "        end0 = self.smooth0(concat0)\n",
    "\n",
    "        return self.final(end0), end2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "nClasses = 2\n",
    "model = UNetVgg(nClasses).to(device)\n",
    "model_name = 'cleanner.pth'\n",
    "\n",
    "\n",
    "seg_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "        torch.nn.ReLU(True),\n",
    "        torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n",
    "        torch.nn.ReLU(True),\n",
    "        torch.nn.Conv2d(128, nClasses, kernel_size=1, stride=1, padding=0)\n",
    "        ).to(device)\n",
    "seg_name = 'seg.pth'\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "except:\n",
    "    raise \"Unable to load model\"\n",
    "model = model.eval()\n",
    "\n",
    "try:\n",
    "    seg_model.load_state_dict(torch.load(seg_name))\n",
    "except:\n",
    "    raise \"Unable to load model\"\n",
    "    \n",
    "seg_model = seg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/198.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/144.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/29.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/95.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/51.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/110.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/122.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/66.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/53.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/153.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/161.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/12.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/9.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/56.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/132.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/36.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/120.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/81.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/44.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/174.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/207.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/15.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/77.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/186.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/134.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/18.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/90.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/162.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/101.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/197.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/195.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/104.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/69.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/17.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/210.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/6.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/105.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/135.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/23.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/185.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/24.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/87.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/213.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/147.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/26.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/203.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/11.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/209.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/93.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/117.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/182.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/33.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/128.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/89.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/177.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/191.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/143.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/86.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/78.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/171.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/149.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/156.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/63.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/173.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/140.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/183.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/167.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/137.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/194.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/170.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/188.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/84.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/155.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/21.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/35.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/45.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/57.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/3.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/113.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/146.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/107.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/30.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/68.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/75.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/216.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/71.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/72.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/125.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/32.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/204.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/99.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/102.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/201.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/48.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/165.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/176.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/74.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/8.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/111.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/215.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/39.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/189.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/126.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/2.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/131.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/5.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/152.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/158.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/54.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/108.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/141.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/159.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/38.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/123.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/20.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/114.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/27.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/129.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/206.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/179.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/59.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/98.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/92.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/119.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/150.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/62.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/42.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/192.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/116.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/14.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/47.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/83.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/164.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/212.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/80.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/60.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/200.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/180.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/41.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/138.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/96.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/65.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/50.png',\n",
       " '/personal/nuveo_test/cv-test/candidate-data/01-DocumentCleanup/noisy_data/168.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files = []\n",
    "for ext in ['png', 'jpeg', 'jpg', 'tif']:\n",
    "    list_files += glob.glob(data_dir +'/*.' + ext)\n",
    "list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo a função que usa os modelos propostos para fazer a inferencia do texto limpo.  \n",
    "As imagens de entrada são invertidas de forma que os caracters ocupem os pixels com maior valor numérico, este é o unico preprocessamento realizado nos dados.  \n",
    "O parametro with_seg_attention diz se vamos utilizar a segmentação de area de texto como uma espécie de \"visual attention\" para a segmentação de caracters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_to_image(net, np_image, with_seg_attention=True):\n",
    "    \n",
    "    if len(np_image.shape) == 2:\n",
    "        np_image = np.dstack((np_image, np_image, np_image))\n",
    "#     np_image = cv2.normalize(np_image,  None, 0, 255, cv2.NORM_MINMAX)\n",
    "    np_image = 255 - np_image\n",
    "    \n",
    "    np_image = np_image.transpose(2, 0, 1).astype('f4')\n",
    "    np_image = np_image[np.newaxis,...]\n",
    "    \n",
    "    label_out, seg_feature = model(torch.from_numpy(np_image).float().to(device))\n",
    "\n",
    "    seg_out = seg_model(seg_feature)\n",
    "\n",
    "\n",
    "    _,_,H,W = label_out.size()\n",
    "#     print(H,W)\n",
    "    seg_out = torch.nn.functional.interpolate(seg_out, size=(H,W), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    attention = torch.nn.functional.softmax(seg_out, dim = 1)\n",
    "    channelA = label_out[:,1,:,:] * attention[:,1,:,:]\n",
    "\n",
    "    channelB = label_out[:,0,:,:] + attention[:,0,:,:]\n",
    "    label_out  = torch.stack((channelB, channelA), 1)\n",
    "    \n",
    "    label_out = torch.nn.functional.softmax(label_out, dim = 1)\n",
    "    label_out = label_out.cpu().detach().numpy()\n",
    "    label_out = np.squeeze(label_out)\n",
    "\n",
    "    seg_out = attention.cpu().detach().numpy()\n",
    "    seg_out = np.squeeze(seg_out)  \n",
    "    \n",
    "    label_out = np.argmax(label_out, axis=0).astype(np.uint8) \n",
    "    seg_out = np.argmax(seg_out, axis=0).astype(np.uint8) \n",
    "    \n",
    "    if with_seg_attention:\n",
    "        label_out = label_out * seg_out\n",
    "\n",
    "    return (1-label_out) * 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas funções foram retiradas do site PyImageSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference PyImageSearch\n",
    "\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "    meanr = np.mean(rect, axis=0)\n",
    "    crect = rect - meanr\n",
    "    crect = crect * 1.05\n",
    "    \n",
    "    rect= crect + meanr\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle_from_principal_component(np_image):\n",
    "    points = np.argwhere(np_image == 0)\n",
    "\n",
    "    mean_points = points.mean(axis = 0)\n",
    "\n",
    "    center_data = points - mean_points\n",
    "\n",
    "    covm = center_data.T.dot(center_data)  * 1.0/float(center_data.shape[0] - 1)\n",
    "\n",
    "    ev , eig = np.linalg.eig(covm)\n",
    "    top_eig = eig[:,np.argmax(ev)]\n",
    "    ang = np.arctan(top_eig[0]/top_eig[1])\n",
    "    return ang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo acontece o processamento de cada uma das images. Note que o modelo é aplicado uma primeira vez onde ambos os modelos são utilizados tanto o segmenta os caracters quanto o que segmenta blocos de texto, na segunda vez apenas o medelo principal que segmenta os caracters é utilizado.  \n",
    "Aqui foi utilizado a função do OpenCV cv2.minAreaRect como fonte dos pontos utilizados para fazer o Warp da imagem essa função retorna um retangulo com um angulo de rotação. Acontece que em muitos casos o texto ocupa uma região que é semelhante a um paralelograma o que faz com que o resultado não fique perfeito.  \n",
    "Aqui para se faz necessário a criação de uma função de fit de um quadritero arbitrário o que deixaria o resultado ainda melhor. Eu optei por não implementar essa função apenas por conta do tempo que eu tinha disponivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(net, np_image):\n",
    "    rst = apply_model_to_image(net, np_image)\n",
    "    \n",
    "#     ang = get_angle_from_principal_component(rst)\n",
    "#     rot_image = ndimage.rotate(np_image, np.rad2deg(ang))\n",
    "    \n",
    "#     rst = apply_model_to_image(net, rot_image)\n",
    "    \n",
    "    kernel = np.ones((7,2),np.uint8)\n",
    "    closing = cv2.morphologyEx(rst, cv2.MORPH_CLOSE, kernel)\n",
    "    closing = cv2.morphologyEx(closing, cv2.MORPH_CLOSE, kernel)\n",
    "#     plt.imshow(closing)\n",
    "#     plt.show()\n",
    "    img_points = (~(closing == 255)*255).astype('u1')\n",
    "    img_points[:10,...] = 0 \n",
    "    img_points[-10:,...] = 0 \n",
    "    img_points[..., :10] = 0 \n",
    "    img_points[..., -10:] = 0\n",
    "    \n",
    "    points = np.argwhere(img_points == 255)\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "    cnt = points[hull.vertices]\n",
    "    cnt = cnt[...,::-1]\n",
    "\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    \n",
    "    np_image = four_point_transform(np_image, box)\n",
    "    \n",
    "    \n",
    "    rst = apply_model_to_image(net, np_image, False)\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processa cada uma das imagens e salva os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "for img in list_files:\n",
    "    np_image = np.asarray(Image.open(img))\n",
    "    rst = process_image(model, np_image)\n",
    "    file_name = os.path.basename(img)\n",
    "    Image.fromarray(rst).save(os.path.join(out_dir , file_name) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
