{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Document Cleanup</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victor Francisco\n",
    "email: victorfco27@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief approach overview\n",
    "\n",
    "I decided to use a few of the most common techniques in image processing area. I also went further and tried to create a CNN to encode and decode the noisy image according to the filtered images. The ideia of using CNN is to increase text resolution since the combination of many filters can degradate image text quality.\n",
    "\n",
    "Remember of creating a folder to output the cleaned images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/victorfco/.virtualenvs/glucogear/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import scipy.misc\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import gaussian, threshold_otsu, median\n",
    "from skimage.feature import canny\n",
    "from skimage.morphology import disk, dilation, erosion\n",
    "from skimage.transform import probabilistic_hough_line, rotate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input, GlobalAveragePooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pad surrounding the image\n",
    "\n",
    "Parameters: array corresponding to the image, integer representing size in pixels for the final height and width, and a boolean to choose the padding color (white or black).\n",
    "\n",
    "Returns: array corresponding to the padded image.\n",
    "\n",
    "This function creates a border in the image in order to make it squared (same height and width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_img(img, final_size, padding_black = False):\n",
    "    h_pad = final_size - img.shape[0]\n",
    "    w_pad = final_size - img.shape[1]\n",
    "    \n",
    "    \n",
    "    # Calculating pad size to apply on image based on desired size and image real size\n",
    "    top = h_pad/2\n",
    "    if top.is_integer():\n",
    "        bottom = top\n",
    "    else:\n",
    "        top = math.ceil(top)\n",
    "        bottom = math.floor(top)-1\n",
    "        \n",
    "    left = w_pad/2\n",
    "    if left.is_integer():\n",
    "        right = left\n",
    "    else:\n",
    "        left = math.ceil(left)\n",
    "        right = math.floor(left)-1\n",
    "\n",
    "    color = [255, 255, 255]    \n",
    "    if padding_black:\n",
    "        color = [0, 0, 0]\n",
    "\n",
    "    new_img = cv.copyMakeBorder(img, int(top), int(bottom), int(left), int(right), cv.BORDER_CONSTANT, value = color)\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fix text rotation\n",
    "\n",
    "Parameters: array representing the image.\n",
    "\n",
    "Returns: array of fixed image.\n",
    "\n",
    "This function makes use of threshold to remove some noises on the image, gaussian filter blurring the text so canny algorithm identify edges. Then, Hough transform to identify lines within image and then histogram to identify most common angles so we can calculate the necessary rotation angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rotation(img):\n",
    "\n",
    "    #threshold to get rid of extraneous noise\n",
    "    thresh = threshold_otsu(img)\n",
    "    normalize = img > thresh\n",
    "\n",
    "    # gaussian blur\n",
    "    blur = gaussian(normalize, 3)\n",
    "\n",
    "    # canny edges in scikit-image\n",
    "    edges = canny(blur)\n",
    "\n",
    "    # hough lines\n",
    "    hough_lines = probabilistic_hough_line(edges)\n",
    "\n",
    "    # hough lines returns a list of points, in the form ((x1, y1), (x2, y2))\n",
    "    # representing line segments. the first step is to calculate the slopes of\n",
    "    # these lines from their paired point values\n",
    "    slopes = [(y2 - y1)/(x2 - x1) if (x2-x1) else 0 for (x1,y1), (x2, y2) in hough_lines]\n",
    "\n",
    "    # it just so happens that this slope is also y where y = tan(theta), the angle\n",
    "    # in a circle by which the line is offset\n",
    "    rad_angles = [np.arctan(x) for x in slopes]\n",
    "\n",
    "    # and we change to degrees for the rotation\n",
    "    deg_angles = [np.degrees(x) for x in rad_angles]\n",
    "\n",
    "    # which of these degree values is most common?\n",
    "    histo = np.histogram(deg_angles, bins=180)\n",
    "    \n",
    "    # correcting for 'sideways' alignments\n",
    "    rotation_angle = histo[1][np.argmax(histo[0])]\n",
    "\n",
    "    if rotation_angle > 45:\n",
    "        rotation_angle = -(90-rotation_angle)\n",
    "    elif rotation_angle < -45:\n",
    "        rotation_angle = 90 - abs(rotation_angle)\n",
    "        \n",
    "    center = (img.shape[0] // 2, img.shape[1] // 2)\n",
    "    rotation_matrix = cv.getRotationMatrix2D(center, round(rotation_angle, 2), 1.0)\n",
    "    rotated_img = cv.warpAffine(img, rotation_matrix, (img.shape[0], img.shape[1]),\n",
    "                                flags=cv.INTER_CUBIC, borderMode=cv.BORDER_CONSTANT,\n",
    "                               borderValue=255)\n",
    "\n",
    "    return rotated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes the background for cleaner image\n",
    "\n",
    "Parameters: image as an array\n",
    "\n",
    "Returns: image array with subtracted background\n",
    "\n",
    "Applying a median filter we can make the text blend with the background, then we subtract the background from the actual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_removal(img):\n",
    "    \n",
    "    # uses a median filter to remove text from image, resulting in only background (noise)\n",
    "    filtered_img = median(img, disk(9))\n",
    "    \n",
    "    # subtracting background from original image\n",
    "    subtracted_img = cv.subtract(filtered_img, img)\n",
    "\n",
    "    # cv.subtract results in a negative image, so we need to return it to white background and black text\n",
    "    subtracted_img = ~subtracted_img\n",
    "    \n",
    "    return subtracted_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusts the image contrast\n",
    "\n",
    "Parameters: image as array and an integer representing the level of contrast.\n",
    "\n",
    "Returns: array reprensenting the image with contrast correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_adjustment(img, level):\n",
    "    img = Image.fromarray(img)\n",
    "    factor = (259 * (level + 255)) / (255 * (259 - level))\n",
    "    def contrast(c):\n",
    "        return 128 + factor * (c - 128)\n",
    "    return np.array(img.point(contrast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image manipulation\n",
    "\n",
    "Open the noisy image from folder and apply the image processing functions to it. Then creates a list of noisy and cleaned images to use in the Convolution Neural Network. In this case, we also apply padding and fix rotation to noisy images to use as input to the CNN.\n",
    "\n",
    "Note: It is recommended to set img_height and img_width to values greater or equal to the max height or width among all input images.\n",
    "\n",
    "\n",
    "DO NOT FORGET OF CREATING A FOLDER FOR CLEAN IMAGES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_path = '/media/victorfco/Storage/Projects/Data/candidate-data/01-DocumentCleanup/noisy_data'\n",
    "clean_path = '/media/victorfco/Storage/Projects/Data/candidate-data/01-DocumentCleanup/clean_data'\n",
    "\n",
    "noisy_imgs = glob.glob(noisy_path + \"/*\" )\n",
    "clean_list = []\n",
    "noisy_list = []\n",
    "\n",
    "# Define the size for your image after padding. HEIGHT and WIDTH MUST BE THE SAME\n",
    "# Chose values greater or equal to the max height or width among all input images\n",
    "img_height = 640\n",
    "img_width = 640\n",
    "\n",
    "\n",
    "for image in noisy_imgs:\n",
    "    img_file = image.split('/')[-1]\n",
    "    noisy = imread(image, as_gray = True)\n",
    "    clean = padding_img(noisy, img_height, padding_black = False)\n",
    "    clean = fix_rotation(clean)\n",
    "    clean = background_removal(clean)\n",
    "    \n",
    "    noisy = padding_img(noisy, img_height, padding_black = False)\n",
    "    noisy = fix_rotation(noisy)\n",
    "    \n",
    "    # Append clean and noisy images with propoer shape with 1 color channel for CNN\n",
    "    clean_list.append(np.stack((clean,)*1, axis = -1))\n",
    "    noisy_list.append(np.stack((noisy,)*1, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = np.asarray(clean_list)\n",
    "noisy_list = np.asarray(noisy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = clean_list/255\n",
    "noisy_list = noisy_list/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 640, 640, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN to improve text in the final image\n",
    "\n",
    "The usage of CNN in this case is to try to improve text resolution. The ideia is to use a simple CNN to encode and decode an image. This way we expect to be able to easily read the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape):   \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    cnn = Conv2D(64, (3,3), activation = 'relu', strides=1, padding='same')(input_layer) \n",
    "    cnn = MaxPooling2D((2, 2), padding='same')(cnn)\n",
    "    \n",
    "    cnn = Conv2D(64, (3, 3), activation = 'relu', strides=1, padding='same')(cnn)\n",
    "    cnn = UpSampling2D((2,2))(cnn)\n",
    "    output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(cnn)\n",
    "        \n",
    "    cnn = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to computational limitations, the batch size is set as 4. Feel free to adjust this parameter according to your PC computational capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " - 22s - loss: 0.0420\n",
      "Epoch 2/80\n",
      " - 17s - loss: 0.0152\n",
      "Epoch 3/80\n",
      " - 16s - loss: 0.0094\n",
      "Epoch 4/80\n",
      " - 17s - loss: 0.0079\n",
      "Epoch 5/80\n",
      " - 16s - loss: 0.0071\n",
      "Epoch 6/80\n",
      " - 16s - loss: 0.0067\n",
      "Epoch 7/80\n",
      " - 16s - loss: 0.0064\n",
      "Epoch 8/80\n",
      " - 16s - loss: 0.0062\n",
      "Epoch 9/80\n",
      " - 16s - loss: 0.0060\n",
      "Epoch 10/80\n",
      " - 17s - loss: 0.0060\n",
      "Epoch 11/80\n",
      " - 16s - loss: 0.0058\n",
      "Epoch 12/80\n",
      " - 16s - loss: 0.0057\n",
      "Epoch 13/80\n",
      " - 17s - loss: 0.0057\n",
      "Epoch 14/80\n",
      " - 17s - loss: 0.0057\n",
      "Epoch 15/80\n",
      " - 17s - loss: 0.0056\n",
      "Epoch 16/80\n",
      " - 16s - loss: 0.0058\n",
      "Epoch 17/80\n",
      " - 17s - loss: 0.0057\n",
      "Epoch 18/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 19/80\n",
      " - 16s - loss: 0.0056\n",
      "Epoch 20/80\n",
      " - 16s - loss: 0.0057\n",
      "Epoch 21/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 22/80\n",
      " - 17s - loss: 0.0056\n",
      "Epoch 23/80\n",
      " - 16s - loss: 0.0056\n",
      "Epoch 24/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 25/80\n",
      " - 16s - loss: 0.0056\n",
      "Epoch 26/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 27/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 28/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 29/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 30/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 31/80\n",
      " - 17s - loss: 0.0054\n",
      "Epoch 32/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 33/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 34/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 35/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 36/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 37/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 38/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 39/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 40/80\n",
      " - 17s - loss: 0.0054\n",
      "Epoch 41/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 42/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 43/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 44/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 45/80\n",
      " - 17s - loss: 0.0054\n",
      "Epoch 46/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 47/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 48/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 49/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 50/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 51/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 52/80\n",
      " - 17s - loss: 0.0054\n",
      "Epoch 53/80\n",
      " - 17s - loss: 0.0055\n",
      "Epoch 54/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 55/80\n",
      " - 16s - loss: 0.0055\n",
      "Epoch 56/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 57/80\n",
      " - 17s - loss: 0.0054\n",
      "Epoch 58/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 59/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 60/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 61/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 62/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 63/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 64/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 65/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 66/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 67/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 68/80\n",
      " - 17s - loss: 0.0053\n",
      "Epoch 69/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 70/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 71/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 72/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 73/80\n",
      " - 16s - loss: 0.0054\n",
      "Epoch 74/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 75/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 76/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 77/80\n",
      " - 16s - loss: 0.0053\n",
      "Epoch 78/80\n",
      " - 16s - loss: 0.0052\n",
      "Epoch 79/80\n",
      " - 16s - loss: 0.0052\n",
      "Epoch 80/80\n",
      " - 16s - loss: 0.0052\n"
     ]
    }
   ],
   "source": [
    "input_shape = (img_height, img_width, 1)\n",
    "\n",
    "X_train = noisy_list\n",
    "y_train = clean_list\n",
    "\n",
    "classifier = create_cnn(input_shape)   \n",
    "    \n",
    "classifier.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "cnn = classifier.fit(X_train, y_train, epochs = 80, batch_size = 4, shuffle = True, verbose = 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving CNN predicted decoded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 2):\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    img_file = noisy_imgs[i].split('/')[-1]\n",
    "    \n",
    "    # Commented lines are for debugging purposes\n",
    "#     fig = plt.figure()\n",
    "#     fig.set_size_inches(25, 10)\n",
    "\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     image = X_train[i] * 255.0\n",
    "#     image = image.reshape(img_width, img_height)\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     image = y_train[i] * 255.0\n",
    "#     image = image.reshape(img_width, img_height)\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "    predict = classifier.predict(X_train[i].reshape(1, img_width, img_height, 1))\n",
    "    predict = predict * 255\n",
    "    predict = predict.reshape(img_width, img_height)\n",
    "#     plt.imshow(predict, cmap='gray')\n",
    "\n",
    "    # Applying threshold to predicted image and save it as binary (0's and 255's)\n",
    "    rotaded = fix_rotation(predict)\n",
    "    thres = threshold_otsu(rotaded)\n",
    "    normalize = rotaded > thres\n",
    "    normalize = normalize.astype(int)\n",
    "    normalize = normalize * 255\n",
    "    \n",
    "    cv.imwrite('{}/{}'.format(clean_path, img_file), normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
